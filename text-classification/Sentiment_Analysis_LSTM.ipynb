{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMuGRjqcCjGzWH2lEMKq64u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Implementing an LSTM to classify audio"],"metadata":{"id":"U72dBk0Tro7j"}},{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfJYQWk_zDJ6","executionInfo":{"status":"ok","timestamp":1722092768368,"user_tz":420,"elapsed":373,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"d42fe2e0-0e97-4cd1-b186-c919ebd915eb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"code","source":["!pip install torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-ugcXfg0WgQ","executionInfo":{"status":"ok","timestamp":1722092816581,"user_tz":420,"elapsed":48215,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"75a3e736-69c6-4577-fcf5-b34a458f588e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.3.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.3.0->torchtext)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mPK6SHDrkao","executionInfo":{"status":"ok","timestamp":1722092845524,"user_tz":420,"elapsed":28960,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"0576ba2f-ca08-4cf4-ef39-16450d402e94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7a84241ea1b0>"]},"metadata":{},"execution_count":3}],"source":["import torch\n","import pandas as pd\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","SEED = 42\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","torch.manual_seed(42)"]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/emot140.csv\")\n","data = data.drop('Unnamed: 0', axis = 1)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ISi1ASH3wzDN","executionInfo":{"status":"ok","timestamp":1722092854344,"user_tz":420,"elapsed":8823,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"c56db993-0228-4e82-d613-099b429d5651"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   emote                                               text\n","0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1      0  is upset that he can't update his Facebook by ...\n","2      0  @Kenichan I dived many times for the ball. Man...\n","3      0    my whole body feels itchy and like its on fire \n","4      0  @nationwideclass no, it's not behaving at all...."],"text/html":["\n","  <div id=\"df-1e6769aa-67f9-4338-bb8d-207cd70c371e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>emote</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e6769aa-67f9-4338-bb8d-207cd70c371e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1e6769aa-67f9-4338-bb8d-207cd70c371e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1e6769aa-67f9-4338-bb8d-207cd70c371e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d07288e7-9d3c-4b25-a2e6-3815d9940bfe\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d07288e7-9d3c-4b25-a2e6-3815d9940bfe')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d07288e7-9d3c-4b25-a2e6-3815d9940bfe button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import re\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","texties = []\n","labels = []\n","dictionary = {}\n","size = 0\n","\n","num_zeros = 0\n","num_fours = 0\n","lim = 75000\n","\n","def preprocess_text(text: str) -> str:\n","    # remove links\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    # remove special chars and numbers\n","    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n","    # remove stopwords\n","    # 1. tokenize\n","    tokens = nltk.word_tokenize(text)\n","    # 2. check if stopword\n","    tokens = [w.lower() for w in tokens if not w in stopwords.words(\"english\")]\n","    return tokens\n","\n","for index, row in data.iterrows():\n","  # checking if we can append to the end of the list\n","  coolio = False\n","  if (num_zeros == lim and num_fours == lim):\n","    break\n","  if ((row['emote'] == 0 and num_zeros < lim) or (row['emote'] == 4 and num_fours < lim)):\n","    coolio = True\n","  if not coolio:\n","    continue\n","\n","  # processing the damn thing\n","  texts = preprocess_text(row['text'])\n","  words = []\n","  for i in range(len(texts)):\n","    if texts[i] in dictionary:\n","      words.append(dictionary[texts[i]])\n","    else:\n","      size += 1\n","      dictionary[texts[i]] = size\n","      words.append(size)\n","  if len(words) == 0:\n","    continue\n","\n","  # updating the counts\n","  num_zeros += (row['emote'] == 0)\n","  num_fours += (row['emote'] == 4)\n","\n","  # appends\n","  texties.append(torch.tensor(words, dtype = torch.int32))\n","  labels.append(torch.tensor([row['emote']], dtype = torch.int32))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SU2pvhZ83CdX","executionInfo":{"status":"ok","timestamp":1722093123946,"user_tz":420,"elapsed":269605,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"03466a0b-cfc1-411c-b73a-427699f7eb8d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["text = torch.nn.utils.rnn.pad_sequence(texties, batch_first = True)\n","label = torch.stack(labels)\n","tdset = torch.utils.data.TensorDataset(text, label)"],"metadata":{"id":"ljaj3cVS8et-","executionInfo":{"status":"ok","timestamp":1722093124832,"user_tz":420,"elapsed":891,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["MXLEN = 0\n","for i in range(2 * lim):\n","  MXLEN = max(MXLEN, texties[i].size(0))\n","print(MXLEN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEA4FJHhUpGT","executionInfo":{"status":"ok","timestamp":1722093124832,"user_tz":420,"elapsed":8,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"98cd2711-b56c-42d0-8402-b3fe7e1a2820"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["36\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class LSTM_Classifier(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n","    super(LSTM_Classifier, self).__init__()\n","    # Embedding layer converts integer sequences to vector sequences\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    # LSTM layer process the vector sequences\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)\n","    #                     input           output                             bisexual?                      dropout_prob      whether or not batch_size comes first in the tensor.size()\n","    # Dense layer to predict\n","    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","    # Prediction activation function\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, text, text_lengths):\n","    embedded = self.embedding(text) # embedded version\n","    # Thanks to packing, LSTM don't see padding tokens\n","    # and this makes our model better\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted = False)\n","    packed_output, (hidden_state, cell_state) = self.lstm(packed_embedded)\n","    # Concatenating the final forward and backward hidden states\n","    hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n","    dense_outputs = self.fc(hidden)\n","    #Final activation function\n","    outputs = self.sigmoid(dense_outputs)\n","    return outputs\n"],"metadata":{"id":"5Ku_EqhB1BFd","executionInfo":{"status":"ok","timestamp":1722093124832,"user_tz":420,"elapsed":5,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","print(len(tdset))\n","train_len = (int)(0.900 * len(tdset))\n","train_dataset, test_dataset = torch.utils.data.random_split(tdset, [train_len, len(tdset) - train_len])\n","print(train_len, len(tdset) - train_len)\n","\n","b_size = 5\n","\n","train_loader = DataLoader(train_dataset, batch_size = b_size, shuffle = True, pin_memory = True, num_workers = 2)\n","test_loader = DataLoader(test_dataset, batch_size = b_size, shuffle = False, pin_memory = True, num_workers = 2)\n","print(len(train_dataset))\n","print(len(test_dataset))"],"metadata":{"id":"lA9w4k0k-Osg","executionInfo":{"status":"ok","timestamp":1722093124832,"user_tz":420,"elapsed":5,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d92784f4-20a7-4f52-ebe7-e3ea0160404d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["150000\n","135000 15000\n","135000\n","15000\n"]}]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"D58V_1qB_gu1"}},{"cell_type":"code","source":["model = LSTM_Classifier(vocab_size = len(dictionary) + 1,\n","                        embedding_dim = 100,\n","                        hidden_dim = 64,\n","                        output_dim = 5,\n","                        n_layers = 5,\n","                        bidirectional = True,\n","                        dropout = 0.5).to(device)\n","optimizer = optim.Adam(model.parameters(), lr = 0.0005)\n","loss_fn = nn.BCELoss().to(device)\n","epochs = 12\n","\n","for i in range(epochs):\n","  model.train()\n","  train_loss = 0.00\n","  correct = 0.00\n","  total = 0.00\n","  for tweet, emotion in train_loader:\n","    tweet = tweet.cuda()\n","    emotion = emotion.cuda()\n","    text_lengths = []\n","    for j in range(b_size):\n","      goddamn = False\n","      for k in range(MXLEN):\n","        if tweet[j][k] == 0:\n","          text_lengths.append(k)\n","          goddamn = True\n","          break\n","      if goddamn == False:\n","        text_lengths.append(MXLEN)\n","    emotion_ = torch.nn.functional.one_hot(emotion.to(torch.int64), 5).to(torch.float)\n","    emotion_ = emotion_.squeeze(1)\n","    optimizer.zero_grad()\n","    y_pred = model(tweet, text_lengths)\n","    loss = loss_fn(y_pred, emotion_)\n","    loss.backward()\n","    optimizer.step()\n","    train_loss += loss.item()/len(train_loader)\n","    prediction = y_pred.argmax(dim=1)\n","    emotion = emotion.squeeze(1)\n","    correct += prediction.eq(emotion).sum().item()\n","    total += emotion.size(0)\n","  print(correct, total)\n","  print(f\"Epoch: {i+1}/{epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {correct/total:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"CxNCNCKn_QA-","executionInfo":{"status":"error","timestamp":1722094002008,"user_tz":420,"elapsed":877179,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"fc1973e7-3263-4b2b-d606-4c4226afda5f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["94886.0 135000.0\n","Epoch: 1/12, Training Loss: 0.2234, Training Accuracy: 0.7029\n","105736.0 135000.0\n","Epoch: 2/12, Training Loss: 0.1844, Training Accuracy: 0.7832\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a11640c8dbd7>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# Validation Loop"],"metadata":{"id":"xjfbbiReBaG8"}},{"cell_type":"code","source":["model.eval()\n","val_loss = 0.00\n","correct = 0.00\n","total = 0.00\n","\n","for tweet, emotion in test_loader:\n","  tweet = tweet.cuda()\n","  emotion = emotion.cuda()\n","  text_lengths = []\n","  for j in range(b_size):\n","    goddamn = False\n","    for k in range(MXLEN):\n","      if tweet[j][k] == 0:\n","        text_lengths.append(k)\n","        goddamn = True\n","        break\n","    if goddamn == False:\n","      text_lengths.append(MXLEN)\n","  emotion_ = torch.nn.functional.one_hot(emotion.to(torch.int64), 5).to(torch.float)\n","  emotion_ = emotion_.squeeze(1)\n","  with torch.no_grad():\n","    y_pred = model(tweet, text_lengths)\n","  loss = loss_fn(y_pred, emotion_)\n","  val_loss += loss.item()/len(test_loader)\n","  emotion = emotion.squeeze(1)\n","  prediction = y_pred.argmax(dim=1)\n","  correct += prediction.eq(emotion).sum().item()\n","  #print(correct, total)\n","  total += emotion.size(0)\n","\n","print(f\"Validation Loss: {val_loss:.4f}\")\n","print(f\"Validation Accuracy: {correct/total:.4f}\")"],"metadata":{"id":"UwmMdxIPBZz4","executionInfo":{"status":"aborted","timestamp":1722094002008,"user_tz":420,"elapsed":4,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":null,"outputs":[]}]}