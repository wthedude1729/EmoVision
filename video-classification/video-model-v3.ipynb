{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyO0LsUFVsil6HTT1/VC60Lg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"DxbAf-tatsE7"}},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27YEMAM2tW1E","executionInfo":{"status":"ok","timestamp":1722301474749,"user_tz":420,"elapsed":1209,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"d910da5a-662d-4922-bc0c-9ad0b87f9e53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Data manipulation and analysis\n","import pandas as pd\n","import numpy as np\n","\n","# Data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Machine learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Deep learning with TensorFlow\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n","from tensorflow.keras.utils import to_categorical\n","\n","# Deep learning with PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torchvision import models\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.models import resnet18, ResNet18_Weights\n","\n","# Natural Language Processing (NLP)\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# Miscellaneous\n","import os\n","import re\n","import time\n","import pickle\n","from PIL import Image\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","source":["# Data Import"],"metadata":{"id":"fPGD2kPbtuyn"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","b_size = 10\n","\n","data = torch.load(\"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman.pt\")\n","train_length = 250\n","train_set, val_set = torch.utils.data.random_split(data, [train_length, len(data) - train_length])\n","\n","train_loader = DataLoader(train_set, batch_size = b_size, shuffle = True, pin_memory = True, num_workers = 2)\n","test_loader = DataLoader(val_set, batch_size = b_size, shuffle = False, pin_memory = True, num_workers = 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y9ihcH-7twYH","executionInfo":{"status":"ok","timestamp":1722301474749,"user_tz":420,"elapsed":2,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"0622d04b-e882-4385-f2bc-bf181418e88f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["import os.path\n","\n","frame_process = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n","])\n","\n","def crop(image):\n","  return image.crop((80, 58, 577, 428))\n","\n","spect_process = transforms.Compose([\n","    transforms.Lambda(crop),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","frame_directory = \"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman6_split/\"\n","spect_directory = \"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman6_spectro/\"\n","folders = [\"anger/\", \"disgust/\", \"fear/\", \"joy/\", \"sadness/\", \"surprise/\"]\n","df = pd.read_csv(\"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman6_texts/en_transcripts.csv\")\n","\n","def preprocess_text(text: str) -> str:\n","    # remove links\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    # remove special chars and numbers\n","    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n","    # remove stopwords\n","    # 1. tokenize\n","    tokens = nltk.word_tokenize(text)\n","    # 2. check if stopword\n","    tokens = [w.lower() for w in tokens if not w in stopwords.words(\"english\")]\n","    return tokens\n","\n","def get_frame_tensor(i):\n","  frames = []\n","  last_valid_filename = \"\"\n","  for j in range(1, 11):\n","    filename = str(i) + \"_\" + str(j) + \".jpg\"\n","    if not os.path.isfile(frame_directory + folders[(int)(i/50)] + filename):\n","      filename = last_valid_filename\n","    if filename == \"\":\n","      print(str(i) + \"_\" + str(j) + \".jpg\")\n","    file = Image.open(frame_directory + folders[(int)(i/50)] + filename)\n","    file = frame_process(file)\n","    frames.append(file)\n","    last_valid_filename = filename\n","  frames = torch.stack(frames)\n","  return frames\n","\n","def get_spect_tensor(i):\n","  filename = str(i) + \".jpg\"\n","  file = Image.open(spect_directory + folders[(int)(i/50)] + filename)\n","  file = spect_process(file)\n","  return file\n","\n","dictionary = {\n","    'EMPTY': 1 # EMPTY --> signal that the text is empty and contains nothing\n","}\n","\n","def get_text_tensor(i):\n","  text = df.columns[i]\n","  text = preprocess_text(text)\n","  liszt = [] # the processed version of the text\n","  for i in range(len(text)):\n","    if text[i] in dictionary:\n","      liszt.append((int)(dictionary[text[i]]))\n","    else:\n","      size = len(dictionary) + 1\n","      dictionary[text[i]] = size\n","      liszt.append((int)(dictionary[text[i]]))\n","  return torch.Tensor(liszt).to(device).to(torch.int64), len(text)\n"],"metadata":{"id":"XTBXvdr6txoY","executionInfo":{"status":"ok","timestamp":1722301474749,"user_tz":420,"elapsed":1,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["text_tensors = []\n","text_lengths = []\n","MXLEN = 0\n","\n","num_input_videos = 300\n","\n","for i in range(num_input_videos):\n","  tt, text_length = get_text_tensor(i)\n","  text_tensors.append(tt)\n","  text_lengths.append(text_length)\n","  MXLEN = max(MXLEN, (int)(tt.size(0)))\n","\n","text_tensors = torch.nn.utils.rnn.pad_sequence(text_tensors, batch_first = True)"],"metadata":{"id":"7thKjIT8tyt7","executionInfo":{"status":"ok","timestamp":1722301480717,"user_tz":420,"elapsed":5969,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["#frame_tensors = []\n","spect_tensors = []\n","\n","for i in range(300):\n","  #print(i)\n","  #frame_tensors.append(get_frame_tensor(i))\n","  spect_tensors.append(get_spect_tensor(i))"],"metadata":{"id":"KRXLCVVzt0A-","executionInfo":{"status":"ok","timestamp":1722301482706,"user_tz":420,"elapsed":1992,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# Reading the Frames"],"metadata":{"id":"ngnA3D_JGNxq"}},{"cell_type":"code","source":["ds = torch.load(\"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/frame.pt\")\n","print(ds.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wdynrYA_GRyC","executionInfo":{"status":"ok","timestamp":1722301482706,"user_tz":420,"elapsed":5,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"b4a16478-ee10-4d85-beec-df8f0370978d"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([300, 10, 3, 32, 32])\n"]}]},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"DB35tIjUt1GA"}},{"cell_type":"markdown","source":["LSTM"],"metadata":{"id":"lokluR2Vt5ku"}},{"cell_type":"code","source":["class LSTM_Classifier(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n","    super(LSTM_Classifier, self).__init__()\n","    # Embedding layer converts integer sequences to vector sequences\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    # LSTM layer process the vector sequences\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)\n","    #                     input           output                             bisexual?                      dropout_prob      whether or not batch_size comes first in the tensor.size()\n","    # Dense layer to predict\n","    self.fc = nn.Linear(hidden_dim * (2 if bidirectional == True else 1), output_dim)\n","    # Prediction activation function\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, text, text_lengths):\n","    embedded = self.embedding(text) # embedded version\n","    # Thanks to packing, LSTM don't see padding tokens\n","    # and this makes our model better\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted = False)\n","    packed_output, (hidden_state, cell_state) = self.lstm(packed_embedded)\n","    # Concatenating the final forward and backward hidden states\n","    hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n","    dense_outputs = self.fc(hidden)\n","    #Final activation function\n","    dense_outputs = nn.functional.relu(dense_outputs)\n","    return dense_outputs"],"metadata":{"id":"fzNa7MSNt35b","executionInfo":{"status":"ok","timestamp":1722301482707,"user_tz":420,"elapsed":4,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["3D-CNN"],"metadata":{"id":"uXffinHqt6Wu"}},{"cell_type":"code","source":["class CNN3d(nn.Module):\n","  def __init__(self):\n","    super(CNN3d, self).__init__()\n","    self.c1 = nn.Conv3d(3, 6, kernel_size = 4, padding = 1, stride = 2)\n","    self.c2 = nn.Conv3d(6, 16, kernel_size = 4, padding = 1, stride = 2)\n","    # 16 x 2 x 8 x 8\n","    self.flatten = nn.Flatten()\n","    self.fc = nn.Sequential(\n","      nn.Linear(16 * 2 * 8 * 8, 128),\n","      nn.Linear(128, 64),\n","      nn.Linear(64, 6),\n","      nn.ReLU()\n","    )\n","\n","  def forward(self, x):\n","    x = self.c1(x)\n","    x = self.c2(x)\n","    x = nn.functional.relu(x)\n","    x = self.flatten(x)\n","    x = self.fc(x)\n","    return x"],"metadata":{"id":"NLgsqwEgt7TX","executionInfo":{"status":"ok","timestamp":1722301482707,"user_tz":420,"elapsed":4,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["Combiner Model"],"metadata":{"id":"b3WnRfahuIZI"}},{"cell_type":"code","source":["class Smash(nn.Module):\n","  def __init__(self):\n","    super(Smash, self).__init__()\n","    self.fcnn = nn.Sequential(\n","      nn.Linear(18, 12),\n","      nn.ReLU(),\n","      nn.Linear(12, 6)\n","    )\n","    self.bilstm = LSTM_Classifier(vocab_size = len(dictionary) + 1,\n","                         embedding_dim = 100,\n","                         hidden_dim = 64,\n","                         output_dim = 6,\n","                         n_layers = 5,\n","                         bidirectional = True,\n","                         dropout = 0.5).to(device)\n","    self.resn = resnet18(num_classes = 6)\n","    self.cnn = CNN3d()\n","\n","  def forward(self, x1, x2, tlengths, x3):\n","    x1 = self.resn(x1)\n","    x2 = self.bilstm(x2, tlengths)\n","    x3 = self.cnn(x3)\n","    x1 = torch.cat((x1, x2, x3), dim = -1)\n","    x1 = self.fcnn(x1)\n","    return x1"],"metadata":{"id":"Kd4yN2lKuL5h","executionInfo":{"status":"ok","timestamp":1722301482707,"user_tz":420,"elapsed":3,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"cOsyM36xuXRe"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","model = Smash().to(device)\n","\n","epochs = 25\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","tl = []\n","\n","for i in range(epochs):\n","  model.train()\n","  train_loss = 0.00\n","  correct = 0.00\n","  total = 0.00\n","  for indices, label in train_loader:\n","    indices = indices.cuda()\n","    label = label.cuda()\n","\n","    # getting the text for the index-th video\n","    text_batch = []\n","    for index in indices:\n","      text = text_tensors[index].to(device)\n","      # if the text ends up being nothing\n","      if text_lengths[index] == 0:\n","        text[0] = dictionary['EMPTY']\n","        # dimensionality voodoo\n","        text_lengths[index] = 1\n","      text_batch.append(text)\n","    text_batch = torch.stack(text_batch)\n","    text_batch = text_batch.squeeze(1)\n","\n","    # getting the spectrograms\n","    spect_batch = []\n","    for index in indices:\n","      spect = spect_tensors[index].to(device)\n","      spect.requires_grad = True\n","      spect_batch.append(spect)\n","    spect_batch = torch.stack(spect_batch)\n","\n","    # getting all of the frames\n","    frame_batch = []\n","    for index in indices:\n","      frames = ds[index.cpu()].to(device)\n","      frames = frames.squeeze(0)\n","      #print(frames.size())\n","      frame_batch.append(frames)\n","    frame_batch = torch.stack(frame_batch)\n","    frame_batch = torch.transpose(frame_batch, 1, 2)\n","    #print(frame_batch.size())\n","\n","    # getting the text lengths\n","    textl = []\n","    for index in indices:\n","      textl.append(text_lengths[index])\n","\n","    # zero-ing gradients\n","    optimizer.zero_grad()\n","\n","    # output of the final NN on the super-tensor\n","    y_pred = model(spect_batch, text_batch, textl, frame_batch).to(device)\n","\n","    # creating one-hot vector for the label for the index-th video\n","    labels = torch.full((b_size, 6,), 0.00).cuda()\n","    for j in range(b_size):\n","      labels[j][label[j]] = 1.00\n","\n","    # computing the Cross Entropy Loss and backpropagating\n","    loss = loss_fn(y_pred, labels)\n","    loss.backward()\n","\n","    # updating gradients\n","    optimizer.step()\n","\n","    # statistics\n","    train_loss += loss.item()/len(train_loader)\n","    prediction = y_pred.argmax(dim=1)\n","    label = label.squeeze(1)\n","    correct += (prediction.eq(label).sum()).item()\n","    total += label.size(0)\n","\n","  print(correct, total)\n","  # more statistics\n","  tl.append(train_loss)\n","  print(f\"Epoch: {i+1}/{epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {correct/total:.4f}\")"],"metadata":{"id":"pTWQAtlDubBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722301694181,"user_tz":420,"elapsed":103049,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"dc8c97c7-bab6-40aa-84ec-afbecfbdbf1f"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["57.0 250.0\n","Epoch: 1/25, Training Loss: 1.7649, Training Accuracy: 0.2280\n","77.0 250.0\n","Epoch: 2/25, Training Loss: 1.6566, Training Accuracy: 0.3080\n","89.0 250.0\n","Epoch: 3/25, Training Loss: 1.5383, Training Accuracy: 0.3560\n","107.0 250.0\n","Epoch: 4/25, Training Loss: 1.3859, Training Accuracy: 0.4280\n","143.0 250.0\n","Epoch: 5/25, Training Loss: 1.2595, Training Accuracy: 0.5720\n","165.0 250.0\n","Epoch: 6/25, Training Loss: 1.1244, Training Accuracy: 0.6600\n","173.0 250.0\n","Epoch: 7/25, Training Loss: 1.0402, Training Accuracy: 0.6920\n","188.0 250.0\n","Epoch: 8/25, Training Loss: 0.9371, Training Accuracy: 0.7520\n","215.0 250.0\n","Epoch: 9/25, Training Loss: 0.7605, Training Accuracy: 0.8600\n","223.0 250.0\n","Epoch: 10/25, Training Loss: 0.6706, Training Accuracy: 0.8920\n","228.0 250.0\n","Epoch: 11/25, Training Loss: 0.5962, Training Accuracy: 0.9120\n","239.0 250.0\n","Epoch: 12/25, Training Loss: 0.4892, Training Accuracy: 0.9560\n","241.0 250.0\n","Epoch: 13/25, Training Loss: 0.4199, Training Accuracy: 0.9640\n","245.0 250.0\n","Epoch: 14/25, Training Loss: 0.3585, Training Accuracy: 0.9800\n","243.0 250.0\n","Epoch: 15/25, Training Loss: 0.3195, Training Accuracy: 0.9720\n","242.0 250.0\n","Epoch: 16/25, Training Loss: 0.2939, Training Accuracy: 0.9680\n","247.0 250.0\n","Epoch: 17/25, Training Loss: 0.2522, Training Accuracy: 0.9880\n","249.0 250.0\n","Epoch: 18/25, Training Loss: 0.1907, Training Accuracy: 0.9960\n","248.0 250.0\n","Epoch: 19/25, Training Loss: 0.1805, Training Accuracy: 0.9920\n","249.0 250.0\n","Epoch: 20/25, Training Loss: 0.1457, Training Accuracy: 0.9960\n","249.0 250.0\n","Epoch: 21/25, Training Loss: 0.1244, Training Accuracy: 0.9960\n","246.0 250.0\n","Epoch: 22/25, Training Loss: 0.1571, Training Accuracy: 0.9840\n","244.0 250.0\n","Epoch: 23/25, Training Loss: 0.1229, Training Accuracy: 0.9760\n","245.0 250.0\n","Epoch: 24/25, Training Loss: 0.1489, Training Accuracy: 0.9800\n","243.0 250.0\n","Epoch: 25/25, Training Loss: 0.1825, Training Accuracy: 0.9720\n"]}]},{"cell_type":"markdown","source":["# Validation Loop"],"metadata":{"id":"SHPrTNrcudTs"}},{"cell_type":"code","source":["model.eval()\n","\n","for indices, label in train_loader:\n","    indices = indices.cuda()\n","    label = label.cuda()\n","\n","    # getting the text for the index-th video\n","    text_batch = []\n","    for index in indices:\n","      text = text_tensors[index].to(device)\n","      # if the text ends up being nothing\n","      if text_lengths[index] == 0:\n","        text[0] = dictionary['EMPTY']\n","        # dimensionality voodoo\n","        text_lengths[index] = 1\n","      text_batch.append(text)\n","    text_batch = torch.stack(text_batch)\n","    text_batch = text_batch.squeeze(1)\n","\n","    # getting the spectrograms\n","    spect_batch = []\n","    for index in indices:\n","      spect = spect_tensors[index].to(device)\n","      spect.requires_grad = True\n","      spect_batch.append(spect)\n","    spect_batch = torch.stack(spect_batch)\n","\n","    # getting all of the frames\n","    frame_batch = []\n","    for index in indices:\n","      frames = ds[index.cpu()].to(device)\n","      frames = frames.squeeze(0)\n","      #print(frames.size())\n","      frame_batch.append(frames)\n","    frame_batch = torch.stack(frame_batch)\n","    frame_batch = torch.transpose(frame_batch, 1, 2)\n","    #print(frame_batch.size())\n","\n","    # getting the text lengths\n","    textl = []\n","    for index in indices:\n","      textl.append(text_lengths[index])\n","\n","    # output of the final NN on the super-tensor\n","    with torch.no_grad():\n","      y_pred = model(spect_batch, text_batch, textl, frame_batch).to(device)\n","\n","    # creating one-hot vector for the label for the index-th video\n","    labels = torch.full((b_size, 6,), 0.00).cuda()\n","    for j in range(b_size):\n","      labels[j][label[j]] = 1.00\n","\n","    train_loss += loss.item()/len(train_loader)\n","    prediction = y_pred.argmax(dim=1)\n","    label = label.squeeze(1)\n","    correct += (prediction.eq(label).sum()).item()\n","    total += label.size(0)\n","\n","print(f\"Validation Accuracy: {correct/total:.4f}\")"],"metadata":{"id":"mqLGHaz-ueVD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722301704324,"user_tz":420,"elapsed":2264,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"cbd3564e-f3d0-439d-a331-02d34853fb47"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.9720\n"]}]},{"cell_type":"code","source":["torch.save(model, \"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/vmodel1.pt\")"],"metadata":{"id":"8VSahyapLsYE","executionInfo":{"status":"ok","timestamp":1722301781902,"user_tz":420,"elapsed":187,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["# Save the Tensor (I do not want to run that cell ever again)"],"metadata":{"id":"Io2tVJkGzAap"}},{"cell_type":"code","source":["frame_tensors = torch.stack(frame_tensors)\n","torch.save(frame_tensors, \"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/frame.pt\")"],"metadata":{"id":"hdG70_L9zDOV","executionInfo":{"status":"error","timestamp":1722301566457,"user_tz":420,"elapsed":8,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"colab":{"base_uri":"https://localhost:8080/","height":159},"outputId":"0d1104f5-7d0b-4be6-ecab-835d70ff0792"},"execution_count":35,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'frame_tensors' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-c9312953c642>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mframe_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/frame.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'frame_tensors' is not defined"]}]}]}