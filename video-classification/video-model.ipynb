{"cells":[{"cell_type":"markdown","metadata":{"id":"vKDPUAqBXkoF"},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36200,"status":"ok","timestamp":1722270346927,"user":{"displayName":"William W.","userId":"17998644639780254164"},"user_tz":420},"id":"dlzxaqpNJx5i","outputId":"4888aed2-62a4-4fdb-fe1d-141423d163f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Data manipulation and analysis\n","import pandas as pd\n","import numpy as np\n","\n","# Data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Machine learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Deep learning with TensorFlow\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n","from tensorflow.keras.utils import to_categorical\n","\n","# Deep learning with PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torchvision import models\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.models import resnet18, ResNet18_Weights\n","\n","# Natural Language Processing (NLP)\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","# Miscellaneous\n","import os\n","import re\n","import time\n","import pickle\n","from PIL import Image\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"8nFTwS-RXmVk"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"RACeNj3rX6EB"},"source":["3D CNN"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":349,"status":"ok","timestamp":1722270370024,"user":{"displayName":"William W.","userId":"17998644639780254164"},"user_tz":420},"id":"OLlzNJ7IXoQX"},"outputs":[],"source":["class CNN3d(nn.Module):\n","  def __init__(self):\n","    super(CNN3d, self).__init__()\n","    self.c1 = nn.Conv3d(3, 6, kernel_size = 4, padding = 1, stride = 2)\n","    self.c2 = nn.Conv3d(6, 16, kernel_size = 10, padding = 0, stride = 2)\n","    # 16 x 3 x 4 x 4 = 256\n","    self.flatten = nn.Flatten()\n","    self.fc = nn.Sequential(\n","      nn.Linear(16 * 3 * 4 * 4, 128),\n","      nn.Linear(128, 64),\n","      nn.Linear(64, 6),\n","      nn.Sigmoid()\n","    )\n","\n","  def forward(self, x):\n","    x = self.c1(x)\n","    x = self.c2(x)\n","    x = nn.functional.relu(x)\n","    x = self.flatten(x)\n","    x = self.fc(x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"btgzfDGdYBM-"},"source":["LSTM"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":350,"status":"ok","timestamp":1722270373453,"user":{"displayName":"William W.","userId":"17998644639780254164"},"user_tz":420},"id":"GOkG_HkjX9QX"},"outputs":[],"source":["class LSTM_Classifier(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n","    super(LSTM_Classifier, self).__init__()\n","    # Embedding layer converts integer sequences to vector sequences\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    # LSTM layer process the vector sequences\n","    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True)\n","    #                     input           output                             bisexual?                      dropout_prob      whether or not batch_size comes first in the tensor.size()\n","    # Dense layer to predict\n","    self.fc = nn.Linear(hidden_dim * (2 if bidirectional == True else 1), output_dim)\n","    # Prediction activation function\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, text, text_lengths):\n","    embedded = self.embedding(text) # embedded version\n","    # Thanks to packing, LSTM don't see padding tokens\n","    # and this makes our model better\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted = False)\n","    packed_output, (hidden_state, cell_state) = self.lstm(packed_embedded)\n","    # Concatenating the final forward and backward hidden states\n","    hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n","    dense_outputs = self.fc(hidden)\n","    #Final activation function\n","    outputs = self.sigmoid(dense_outputs)\n","    return outputs"]},{"cell_type":"markdown","metadata":{"id":"HKhAhcfTYRbp"},"source":["FC NN"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1722270376980,"user":{"displayName":"William W.","userId":"17998644639780254164"},"user_tz":420},"id":"WDvSx7MxYR_-"},"outputs":[],"source":["class Smash(nn.Module):\n","  def __init__(self):\n","    super(Smash, self).__init__()\n","    self.fcnn = nn.Sequential(\n","      nn.Linear(1006, 12),\n","      nn.Linear(12, 6),\n","      nn.Sigmoid()\n","    )\n","\n","  def forward(self, x):\n","    x = self.fcnn(x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"hBbC7DiRixqL"},"source":["# Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4767,"status":"ok","timestamp":1722270384415,"user":{"displayName":"William W.","userId":"17998644639780254164"},"user_tz":420},"id":"mPGwHeaLirWh","outputId":"15f9d794-7649-4416-aad8-1f633fe7187f"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","data = torch.load(\"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman.pt\")\n","train_length = (int)(0.9 * len(data))\n","train_set, val_set = torch.utils.data.random_split(data, [train_length, len(data) - train_length])\n","\n","train_loader = DataLoader(train_set, shuffle = True, pin_memory = True, num_workers = 2)\n","test_loader = DataLoader(val_set, shuffle = False, pin_memory = True, num_workers = 2)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":1128,"status":"ok","timestamp":1722271951983,"user":{"displayName":"William W.","userId":"17998644639780254164"},"user_tz":420},"id":"9ksNODHAkBOY"},"outputs":[],"source":["import os.path\n","\n","frame_process = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor()\n","])\n","\n","spect_process = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","frame_directory = \"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman6_split/\"\n","spect_directory = \"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman6_spectro/\"\n","folders = [\"anger/\", \"disgust/\", \"fear/\", \"joy/\", \"sadness/\", \"surprise/\"]\n","df = pd.read_csv(\"/content/drive/My Drive/Machine Learning/COSMOS/FINAL_PROJECT/DER/ekman6_texts/en_transcripts.csv\")\n","\n","def preprocess_text(text: str) -> str:\n","    # remove links\n","    text = re.sub(r\"http\\S+\", \"\", text)\n","    # remove special chars and numbers\n","    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n","    # remove stopwords\n","    # 1. tokenize\n","    tokens = nltk.word_tokenize(text)\n","    # 2. check if stopword\n","    tokens = [w.lower() for w in tokens if not w in stopwords.words(\"english\")]\n","    return tokens\n","\n","def get_frame_tensor(i):\n","  frames = []\n","  last_valid_filename = \"\"\n","  for j in range(1, 30):\n","    filename = str(i) + \"_\" + str(j) + \".jpg\"\n","    if not os.path.isfile(frame_directory + folders[(int)(i/50)] + filename):\n","      filename = last_valid_filename\n","    if filename == \"\":\n","      print(str(i) + \"_\" + str(j) + \".jpg\")\n","    file = Image.open(frame_directory + folders[(int)(i/50)] + filename)\n","    file = frame_process(file)\n","    frames.append(file)\n","    last_valid_filename = filename\n","  frames = torch.stack(frames)\n","  return frames\n","\n","def get_spect_tensor(i):\n","  filename = str(i) + \".jpg\"\n","  file = Image.open(spect_directory + folders[(int)(i/50)] + filename)\n","  file = spect_process(file)\n","  return file\n","\n","dictionary = {\n","    'EMPTY': 1\n","}\n","\n","def get_text_tensor(i):\n","  text = df.columns[i]\n","  text = preprocess_text(text)\n","  liszt = []\n","  for i in range(len(text)):\n","    if text[i] in dictionary:\n","      liszt.append((int)(dictionary[text[i]]))\n","    else:\n","      size = len(dictionary) + 1\n","      dictionary[text[i]] = size\n","      liszt.append((int)(dictionary[text[i]]))\n","  return torch.IntTensor(liszt).to(device), len(text)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":6152,"status":"ok","timestamp":1722271959980,"user":{"displayName":"William W.","userId":"17998644639780254164"},"user_tz":420},"id":"5BKtw9kMomkx"},"outputs":[],"source":["text_tensors = []\n","text_lengths = []\n","MXLEN = 0\n","\n","for i in range(300):\n","  tt, text_length = get_text_tensor(i)\n","  text_tensors.append(tt)\n","  text_lengths.append(text_length)\n","  MXLEN = max(MXLEN, (int)(tt.size(0)))\n","\n","text_tensors = torch.nn.utils.rnn.pad_sequence(text_tensors, batch_first = True)"]},{"cell_type":"code","source":["frame_tensors = []\n","spect_tensors = []\n","\n","for i in range(300):\n","  #frame_tensors.append(get_frame_tensor(i))\n","  spect_tensors.append(get_spect_tensor(i))"],"metadata":{"id":"2MMeB9fjM_be","executionInfo":{"status":"ok","timestamp":1722270617433,"user_tz":420,"elapsed":203301,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTZ_O7s2ir9k"},"source":["# Training Loop"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0FN8r02YItL","executionInfo":{"status":"ok","timestamp":1722273012839,"user_tz":420,"elapsed":131001,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"2817516b-5637-4425-cd10-ea1207f4ffa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10, Training Loss: 1.9055, Training Accuracy: 0.1704\n","Epoch: 2/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 3/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 4/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 5/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 6/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 7/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 8/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 9/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n","Epoch: 10/10, Training Loss: 1.9045, Training Accuracy: 0.1741\n"]}],"source":["model1 = CNN3d().to(device)\n","model2 = resnet18(pretrained = True).to(device) # output == 1000 neurons\n","model3 = LSTM_Classifier(vocab_size = len(dictionary) + 1,\n","                         embedding_dim = 100,\n","                         hidden_dim = 64,\n","                         output_dim = 6,\n","                         n_layers = 5,\n","                         bidirectional = True,\n","                         dropout = 0.5).to(device)\n","model4 = Smash().to(device)\n","\n","epochs = 10\n","\n","optimizer1 = optim.Adam(model1.parameters(), lr = 0.001)\n","optimizer2 = optim.Adam(model2.parameters(), lr = 0.01)\n","optimizer3 = optim.Adam(model3.parameters(), lr = 0.01)\n","optimizer4 = optim.Adam(model4.parameters(), lr = 0.01)\n","\n","loss_fn = nn.CrossEntropyLoss()\n","\n","tl = []\n","\n","def zerograd():\n","  optimizer1.zero_grad()\n","  optimizer2.zero_grad()\n","  optimizer3.zero_grad()\n","  optimizer4.zero_grad()\n","\n","def step():\n","  optimizer1.step()\n","  optimizer2.step()\n","  optimizer3.step()\n","  optimizer4.step()\n","\n","for i in range(epochs):\n","  model2.train()\n","  model3.train()\n","  model4.train()\n","  train_loss = 0.00\n","  correct = 0.00\n","  total = 0.00\n","  for index, label in train_loader:\n","    index = index.cuda()\n","    label = label.cuda()\n","    text = text_tensors[index].to(device)\n","    if text_lengths[index] == 0:\n","      text = torch.IntTensor([dictionary['EMPTY']]).to(device)\n","      text = text.unsqueeze(0)\n","      text = text.unsqueeze(0)\n","      text_lengths[index] = 1\n","    spect = spect_tensors[index].to(device)\n","    zerograd()\n","    spect = spect.unsqueeze(0)\n","    assert spect.size(0) == 1\n","    y_spect = model2(spect).to(device)\n","    text = text.squeeze(1)\n","    y_text = model3(text, [text_lengths[index]]).to(device)\n","    input = torch.cat((y_spect, y_text), dim=-1)\n","    y_pred = model4(input).to(device)\n","    labels = torch.full((1, 6,), 0.00).cuda()\n","    labels[0][label] = 1.00\n","    loss = loss_fn(y_pred, labels)\n","    loss.backward()\n","    step()\n","    train_loss += loss.item()/len(train_loader)\n","    prediction = y_pred.argmax(dim=1)\n","    correct += (prediction.eq(label).sum()).item()\n","    total += label.size(0)\n","  tl.append(train_loss)\n","  print(f\"Epoch: {i+1}/{epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {correct/total:.4f}\")"]},{"cell_type":"code","source":["#print(model2.weights.grad)\n","print(model3.lstm._parameters['weight_ih_l0'].grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7brhahVRavhf","executionInfo":{"status":"ok","timestamp":1722272619002,"user_tz":420,"elapsed":823,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"d119ae04-5714-4646-9442-79f16205cad6"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.9289e-14, -8.8287e-15, -6.6818e-14,  ..., -8.5873e-14,\n","          3.2373e-14, -6.6934e-14],\n","        [ 5.0574e-14,  2.4564e-14,  7.7181e-14,  ...,  1.0572e-14,\n","          2.0071e-14,  5.8941e-14],\n","        [-1.1497e-15,  1.1921e-14,  1.4711e-15,  ...,  3.6003e-14,\n","          5.3921e-14,  7.1869e-14],\n","        ...,\n","        [ 5.8269e-14,  7.7323e-15, -8.0172e-15,  ..., -1.5655e-14,\n","          6.1100e-15, -4.8381e-14],\n","        [-4.3988e-14, -4.4640e-14,  1.0935e-13,  ...,  8.7645e-14,\n","          2.1820e-15,  9.8212e-14],\n","        [-1.5280e-13, -2.6795e-14,  3.3053e-14,  ..., -3.9506e-14,\n","         -1.0776e-13,  1.4172e-14]], device='cuda:0')\n"]}]},{"cell_type":"markdown","metadata":{"id":"27uNcMn1yvxV"},"source":["# Validation Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wr70yLwAyxrM","executionInfo":{"status":"aborted","timestamp":1722270270197,"user_tz":420,"elapsed":3,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyO3HgKIj9JLzGzfDVRfGBx/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}