{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMBAYNeFwqyeLkghZG90MLx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Mount Google Drive"],"metadata":{"id":"dEFE39aL0hoZ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"hwbm-3ht0ePE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721934484272,"user_tz":420,"elapsed":25924,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"69707d81-6da0-42f6-c2d4-6f5efe178f1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import matplotlib.pyplot as plt\n","from scipy.io import wavfile as wav\n","from scipy.fftpack import fft\n","from scipy import signal\n","import numpy as np\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import librosa.display\n","import librosa.core\n","\n","import numpy as np\n","import pandas as pd\n","import librosa\n","\n","import torch\n","from PIL import Image\n","import torchvision.transforms as transforms # not to be confused with tensorflow\n","from torchvision.transforms.functional import crop\n","from io import BytesIO"]},{"cell_type":"markdown","source":["# Create Spectrogram Images"],"metadata":{"id":"sk07bojD0h82"}},{"cell_type":"code","source":["root = \"/content/drive/My Drive/Machine Learning/COSMOS/Final Project/Emotion Recognition/RAVDESS/\"\n","\n","count = 0\n","\n","for i in range(1, 25):\n","  actor = \"Actor_\" + (\"0\" if i < 10 else \"\") + str(i) + \"/\"\n","  directory = os.fsencode(root + actor)\n","  for file in os.listdir(directory):\n","    filename = os.fsdecode(file)\n","    b = \"/content/drive/My Drive/Machine Learning/COSMOS/Final Project/Emotion Recognition/ravimg/\"\n","    y, sr = librosa.load(root + actor + filename)\n","    y = y[10000:80000] # shorten audio a bit for speed\n","\n","    window_size = 1024\n","    window = np.hanning(window_size)\n","    stft  = librosa.core.spectrum.stft(y, n_fft=window_size, hop_length=512, window=window)\n","    out = 2 * np.abs(stft) / np.sum(window)\n","\n","    # For plotting headlessly\n","    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n","\n","    fig = plt.Figure()\n","    canvas = FigureCanvas(fig)\n","    ax = fig.add_subplot(111)\n","    p = librosa.display.specshow(librosa.amplitude_to_db(out, ref=np.max), ax=ax, y_axis='log', x_axis='time')\n","    emotion = filename[6:8]\n","    fig.savefig(b + emotion + \"_\" + str(count) + \".jpg\")\n","    count += 1"],"metadata":{"id":"bsSDM7r70iaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Dataset"],"metadata":{"id":"N5A5qWeh2DU0"}},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import torchvision.transforms as transforms # not to be confused with tensorflow\n","from torchvision.transforms.functional import crop\n","from io import BytesIO\n","\n","def crop(image):\n","  return image.crop((80, 58, 577, 428))\n","\n","transform = transforms.Compose([\n","    #transforms.ToPILImage(),\n","    transforms.Lambda(crop),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"3cirfZ3R2wWL","executionInfo":{"status":"ok","timestamp":1721934525468,"user_tz":420,"elapsed":225,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Optional Sanity Check (comment out transforms.ToTensor())"],"metadata":{"id":"uI2lGsvB4ib8"}},{"cell_type":"code","source":["import matplotlib.image as mpimg\n","\n","direct = \"/content/drive/My Drive/Machine Learning/COSMOS/Final Project/Emotion Recognition/ravimg/\"\n","cntt = 0\n","for file in os.listdir(direct):\n","  img = mpimg.imread(direct + file)\n","  plt.imshow(img)\n","  plt.show()\n","  with open(direct + file, 'rb') as f:\n","    img = f.read()\n","    image = Image.open(BytesIO(img))\n","    image = transform(image)\n","  plt.imshow(image)\n","  plt.axis('off')\n","  plt.show()\n","  cntt += 1\n","  #print(cntt)\n","  if cntt == 5:\n","    break"],"metadata":{"id":"9YpWQtnt2xl6","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1OkQqJUDwSpNRsDx9-79ptJLP0PTIv7Ri"},"executionInfo":{"status":"ok","timestamp":1721934503599,"user_tz":420,"elapsed":11029,"user":{"displayName":"William W.","userId":"17998644639780254164"}},"outputId":"7062460c-3082-4f8c-fead-4f412ae8787b"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Save to .pt file"],"metadata":{"id":"1kQwhnOt4qxh"}},{"cell_type":"code","source":["root_path = \"/content/drive/My Drive/Machine Learning/COSMOS/Final Project/Emotion Recognition/\"\n","folder = \"ravimg/\"\n","\n","directory = os.fsencode(root_path + folder)\n","count = 0\n","\n","features = []\n","labels = []\n","cnt = 0\n","\n","for file in os.listdir(directory):\n","  filename = os.fsdecode(file)\n","  with open(root_path + folder + filename, 'rb') as f:\n","    img = f.read()\n","    image = Image.open(BytesIO(img))\n","    image = transform(image)\n","    features.append(image)\n","  emote = filename[0:2]\n","  emote = int(emote) - 1\n","  labels.append(torch.tensor([emote]))\n","\n","features = torch.stack(features)\n","labels = torch.stack(labels)\n","dset = torch.utils.data.TensorDataset(features, labels)\n","torch.save(dset, \"dset.pt\")"],"metadata":{"id":"aMGN2BWs4m_8","executionInfo":{"status":"ok","timestamp":1721934758254,"user_tz":420,"elapsed":17283,"user":{"displayName":"William W.","userId":"17998644639780254164"}}},"execution_count":9,"outputs":[]}]}